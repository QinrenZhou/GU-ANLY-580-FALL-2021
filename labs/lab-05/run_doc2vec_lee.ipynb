{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Doc2Vec Model\n",
    "=============\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the\n",
    "`Lee Corpus <https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Review: ``Word2Vec`` Model\n",
    "--------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "Introducing: Paragraph Vector\n",
    "-----------------------------\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__,\n",
    "which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "Prepare the Training and Test Data\n",
    "----------------------------------\n",
    "\n",
    "For this tutorial, we'll be training our model using the `Lee Background\n",
    "Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporation’s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter `Lee Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 15:13:37,755 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to Read and Preprocess Text\n",
    "---------------------------------------------\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hundreds of people have been forced to v\n",
      "1 Indian security forces have shot dead ei\n",
      "2 The national road toll for the Christmas\n",
      "3 Argentina's political and economic crisi\n",
      "4 Six midwives have been suspended at Woll\n",
      "5 The Federal Government says it should be\n",
      "6 The United States team of Monica Seles a\n",
      "7 Hundreds of canoeists are enjoying hard-\n",
      "8 There has been welcome relief for firefi\n",
      "9 Some roads are closed because of dangero\n",
      "10 Work is continuing this morning to resto\n",
      "11 Peru has entered two days of official mo\n",
      "12 President General Pervez Musharraf says \n",
      "13 Talks between Afghan and British officia\n",
      "14 The Israeli army has killed three Palest\n",
      "15 Only a protest will can now stop Bumbleb\n",
      "16 South Africa is considering playing left\n",
      "17 Spain has begun its Hopman Cup campaign \n",
      "18 New Zealand has rewarded Lord of the Rin\n",
      "19 The next few hours are crucial for firef\n",
      "20 Argentine President Adolfo Rodriguez Saa\n",
      "21 The nation's road toll has risen to 37, \n",
      "22 The Federal Government says new national\n",
      "23 Americans' fears about airplane security\n",
      "24 A team of police is currently escorting \n",
      "25 The handicap winner of the Melbourne to \n",
      "26 Pakistan's President Pervez Musharraf sa\n",
      "27 Swedish Round the World ocean racer Assa\n",
      "28 A United States federal magistrate has r\n",
      "29 The Palestinian leadership is calling fo\n",
      "30 The Northern Territory Aids Council says\n",
      "31 There is a one in 20 chance of a dramati\n",
      "32 An earthquake measuring 4.1 on the Richt\n",
      "33 New South Wales firefighters are hoping \n",
      "34 Pakistan's Foreign Ministry has announce\n",
      "35 A spokesman for Afghanistan's Defence Mi\n",
      "36 New York Mayor Rudolph Giuliani today ba\n",
      "37 Australia's quicks and opening batsmen h\n",
      "38 A rafter who raised the alarm after most\n",
      "39 A total of 14 yachts have now retired fr\n",
      "40 Firefighters across New South Wales are \n",
      "41 The man accused to trying to blow up an \n",
      "42 Virgin Blue has begun offering $5 flight\n",
      "43 After a bad start to the holiday period \n",
      "44 A Palestinian man has been shot dead as \n",
      "45 Sir Nigel Hawthorne, the British actor b\n",
      "46 Seven yachts have been forced to retire \n",
      "47 Australia will be aiming to take early w\n",
      "48 Thousands of firefighters remain on the \n",
      "49 European monarchs have reflected on the \n",
      "50 Afghan security forces have arrested a w\n",
      "51 Russian authorities have sentenced Chech\n",
      "52 Skippers are expecting a spectacular sta\n",
      "53 A project working on ways to reduce the \n",
      "54 Police are interviewing a 21-year-old ma\n",
      "55 Melbourne's weather is one of the questi\n",
      "56 An American Airlines flight from Paris t\n",
      "57 Afghanistan's new interim government is \n",
      "58 Pakistan President Pervez Musharraf beli\n",
      "59 Australian cricket selectors have made j\n",
      "60 Israel has rejected Palestinian leader Y\n",
      "61 A Victorian couple is seeking approval t\n",
      "62 Japanese officials say their Coast Guard\n",
      "63 Tight security is causing headaches for \n",
      "64 A high profile church leader says the Go\n",
      "65 The Chief of the Army, Lieutenant-Genera\n",
      "66 Argentina's Government has crumbled afte\n",
      "67 A pay freeze dispute involving Qantas an\n",
      "68 After months of delays, the company behi\n",
      "69 An Iraqi defector who has applied for re\n",
      "70 The Queensland Premier says he accepts f\n",
      "71 In the United States, Australian actress\n",
      "72 Australian cricket selectors have made j\n",
      "73 The Prime Minister has thrown his full s\n",
      "74 The United Nations Security Council has \n",
      "75 US President George W Bush has marked th\n",
      "76 The death toll in Argentina's food riots\n",
      "77 The Woomera Detention Centre in outback \n",
      "78 The private business sector has to compl\n",
      "79 Dozens of people were injured, some seri\n",
      "80 Zimbabwe has been given five weeks to st\n",
      "81 A rare calm in the Palestinian territori\n",
      "82 The owner of a nudist resort in South Au\n",
      "83 The Opposition leader, Simon Crean, says\n",
      "84 It has been confirmed two asylum seekers\n",
      "85 Hamas militants have fought gun battles \n",
      "86 Argentina's Economy Minister Domingo Cav\n",
      "87 The Australian Transport Safety Bureau h\n",
      "88 The coroner investigating the death of a\n",
      "89 After the torching of more than 20 build\n",
      "90 Anti-child abuse groups are calling on A\n",
      "91 The Flanders graveyard of thousands of A\n",
      "92 Federal Labor MP Carmen Lawrence says th\n",
      "93 A senior Hamas official has said the rad\n",
      "94 Foreign Minister Alexander Downer says t\n",
      "95 Legal abortion in Tasmania is one step c\n",
      "96 England batsman Michael Vaughan has beco\n",
      "97 Australian authorities are to be granted\n",
      "98 The Pentagon says the US military is con\n",
      "99 The Civil Aviation Safety Authority (CAS\n",
      "100 The Northern Territory's coroner has fou\n",
      "101 Ansett's administrators are confident of\n",
      "102 The secretary general of the Law Council\n",
      "103 The HIH Royal Commission has heard evide\n",
      "104 Australian cricket captain Steve Waugh h\n",
      "105 Fresh palls of smoke are billowing from \n",
      "106 The Federal Government has called on Lab\n",
      "107 The Pentagon says the US military is con\n",
      "108 The International Monetary Fund (IMF) ha\n",
      "109 Fire has damaged part of St John the Div\n",
      "110 The radical Palestinian group Hamas has \n",
      "111 Joseph Gutnick, the saviour and former p\n",
      "112 Australian cricket captain Steve Waugh h\n",
      "113 The Immigration Department says overnigh\n",
      "114 The Federal Cabinet has today endorsed a\n",
      "115 Australia is continuing to negotiate wit\n",
      "116 Yasser Arafat has accused Israel of esca\n",
      "117 Unions representing Qantas maintenance w\n",
      "118 Australia has beaten South Africa by 246\n",
      "119 Australia is continuing to negotiate wit\n",
      "120 Unions representing Qantas maintenance w\n",
      "121 The latest business expectations survey \n",
      "122 At least four people, including two poli\n",
      "123 A new report has revealed there are fewe\n",
      "124 The Federal Opposition wants tougher pen\n",
      "125 The United States Space Shuttle Endeavou\n",
      "126 Federal Science Minister Peter McGauran \n",
      "127 US forces backed by their Afghan allies \n",
      "128 Qantas has moved to assure travellers th\n",
      "129 The Governor-General will issue a statem\n",
      "130 The condition of former Indonesian dicta\n",
      "131 The new Solomon Islands Prime Minister h\n",
      "132 Australia has picked up two wickets in S\n",
      "133 The hunt for Osama bin Laden has shifted\n",
      "134 Israel has reacted with caution to a pro\n",
      "135 A dispute which could threaten air servi\n",
      "136 A new report suggests the costs of an ag\n",
      "137 Striking Latrobe Valley power workers wi\n",
      "138 The members of the newly-elected Solomon\n",
      "139 Australia will be looking to score quick\n",
      "140 Osama bin Laden admitted planning the Se\n",
      "141 United States air strikes on Al Qaeda fi\n",
      "142 The Defence Minister, Robert Hill, says \n",
      "143 Kashmiri militant groups denied involvem\n",
      "144 An investigation is underway into what p\n",
      "145 Israeli helicopter gunships and warplane\n",
      "146 The Australian and South African sides f\n",
      "147 The Federal Government is negotiating wi\n",
      "148 The Israeli Government has declared Pale\n",
      "149 The Federal Opposition says the unemploy\n",
      "150 Industrial action will affect three of A\n",
      "151 Senior Construction Forestry Mining and \n",
      "152 The mind games are continuing as Austral\n",
      "153 At least two helicopters have landed nea\n",
      "154 Australia and the United Nations have op\n",
      "155 The former managing director of One.Tel \n",
      "156 Industrial action will affect three of A\n",
      "157 A British man has been found guilty by a\n",
      "158 The AFL's leading goal kicker, Tony Lock\n",
      "159 The Pentagon believes it has finally con\n",
      "160 A French Moroccan man has been charged i\n",
      "161 Australian families of the victims from \n",
      "162 The Federal Agriculture Minister, Warren\n",
      "163 The secret Australian budget for the boa\n",
      "164 Japanese car maker Mitsubishi, has confi\n",
      "165 Socceroos coach Frank Farina says he cou\n",
      "166 The Federal Government says ASIO and the\n",
      "167 Turning grief into defiance, Americans h\n",
      "168 Six Swiss tour company staff have been f\n",
      "169 A United Nations panel of judges in East\n",
      "170 The United States Federal Reserve has cu\n",
      "171 Drug education campaigns appear to be pa\n",
      "172 The number of adults and children being \n",
      "173 United States peace envoy Anthony Zinni \n",
      "174 Milestones in the history of radio will \n",
      "175 About 60,000 bank staff will walk off th\n",
      "176 Anti-Taliban fighters say they have capt\n",
      "177 Israeli helicopters have again attacked \n",
      "178 A 31-year-old Middle Eastern woman is sa\n",
      "179 Qantas has unveiled Australia's latest a\n",
      "180 Australia has linked $10 million of aid \n",
      "181 The Australian Transport Safety Bureau i\n",
      "182 The Australian cricket team has arrived \n",
      "183 Most of the Tora Bora mountain complex i\n",
      "184 The Israel Government has expressed regr\n",
      "185 Unions are already expressing their diss\n",
      "186 United Nationals secretary-general Kofi \n",
      "187 Qantas maintenance workers will decide b\n",
      "188 One person has died after a Royal Flying\n",
      "189 New statistics released by the Cancer Co\n",
      "190 Conservationists have applauded the one-\n",
      "191 Ian Thorpe has emulated Kieren Perkins f\n",
      "192 The United States is intensifying its bo\n",
      "193 A new study shows that nearly one third \n",
      "194 The Royal Commission into collapsed insu\n",
      "195 Unions representing Qantas maintenance w\n",
      "196 Olympic 400 metres champion Cathy Freema\n",
      "197 The Middle East peace process is under n\n",
      "198 Authorities are trying to track down the\n",
      "199 The Royal Commission into the Building I\n",
      "200 The United States says a video tape foun\n",
      "201 Prime Minister Ariel Sharon said Israel \n",
      "202 An International study has found thousan\n",
      "203 Qantas management and unions representin\n",
      "204 An Iraqi doctor, being held at Sydney's \n",
      "205 Australian's casinos generated a $3.1 bi\n",
      "206 The royal commission into the building i\n",
      "207 Geoff Huegill has continued his record-b\n",
      "208 Israeli tanks and troops have launched t\n",
      "209 The Federal Government has confirmed the\n",
      "210 The Australian Government is continuing \n",
      "211 A gunman has died after he went on a sho\n",
      "212 New Zealand's ambassador to Brazil, Deni\n",
      "213 A Swiss fireman has told a court how he \n",
      "214 The United States offered full and direc\n",
      "215 Jason Stoltenberg will become the new co\n",
      "216 A senior Taliban official confirmed the \n",
      "217 A suspect allegedly involved in planning\n",
      "218 Refugee support groups are strongly crit\n",
      "219 Several people, believed to be as many a\n",
      "220 The armed wing of the radical Islamic mo\n",
      "221 Reserve Bank Governor Ian Macfarlane say\n",
      "222 New laws requiring all packaged food pro\n",
      "223 Indonesian troop re-enforcements have st\n",
      "224 America's Cup winner Sir Peter Blake, on\n",
      "225 The Federal Government says a man who ha\n",
      "226 The three US soldiers killed by a misgui\n",
      "227 A tense stand-off is continuing in Gaza \n",
      "228 The Immigration Minister, Philip Ruddock\n",
      "229 The Federal Education Minister, Brendan \n",
      "230 Two Swiss guides who survived the 1999 I\n",
      "231 The US space shuttle Endeavour has blast\n",
      "232 Australian swimmers have won nine of the\n",
      "233 Three US troops and five members of the \n",
      "234 The Foreign Minister, Alexander Downer, \n",
      "235 Israel has demanded the arrest of 36 Pal\n",
      "236 Two Swiss guides who survived the 1999 I\n",
      "237 High interest rates on credit cards have\n",
      "238 Centrelink is urging people affected by \n",
      "239 The Department of Foreign Affairs and Tr\n",
      "240 Counting is proceeding very slowly in th\n",
      "241 Australian swimmers have won nine of the\n",
      "242 There has been another suicide bomb atta\n",
      "243 Four Afghan factions have reached agreem\n",
      "244 The Reserve Bank has cut official intere\n",
      "245 Federal Treasurer Peter Costello has war\n",
      "246 The AFL's all-time leading goalkicker, T\n",
      "247 The Royal Commission into HIH has been a\n",
      "248 Darwin Aboriginal custodians will become\n",
      "249 The Defence Minister, Robert Hill, has a\n",
      "250 Israel launched massive air raids across\n",
      "251 Interest rates and economic growth take \n",
      "252 The Labor Party is set to have a wide-ra\n",
      "253 Computer virus experts have warned of a \n",
      "254 Defendants in the Interlaken canyoning t\n",
      "255 The New South Wales State Emergency Serv\n",
      "256 A survey of literacy and mathematical sk\n",
      "257 Hundreds of fans stood vigil today for t\n",
      "258 Australia has escaped with a draw after \n",
      "259 Israeli forces have launched attacks on \n",
      "260 Traveland's wholly-owned travel centres \n",
      "261 Afghan opposition leaders meeting in Ger\n",
      "262 At the royal commission inquiry into the\n",
      "263 A director of a defunct Swiss company th\n",
      "264 Widespread damage from yesterday's viole\n",
      "265 The Federal Government is under fire fro\n",
      "266 Australian fast bowler Brett Lee has bee\n",
      "267 Israeli Prime Minister Ariel Sharon has \n",
      "268 Opposition forces claimed to have captur\n",
      "269 The Prime Minister, John Howard, has rev\n",
      "270 Businessmen Solomon Lew and Lindsay Fox \n",
      "271 A director of a defunct Swiss company th\n",
      "272 The storm clean-up in Sydney will resume\n",
      "273 The royal commission into the collapse o\n",
      "274 Around 1,000 people have braved the cold\n",
      "275 Australian cricket coach John Buchanan s\n",
      "276 Defence Minister Robert Hill has confirm\n",
      "277 Israeli soldiers have shot dead five Pal\n",
      "278 The royal commission looking into the co\n",
      "279 Forward indicators of the Australian lab\n",
      "280 The Greens have officially won their sec\n",
      "281 Eight people are to appear in a Swiss co\n",
      "282 The administrator of the financially tro\n",
      "283 France is celebrating victory over Austr\n",
      "284 Secretary of State Colin Powell says the\n",
      "285 A new economic report claims Australia's\n",
      "286 Malaysian police have arrested a man bel\n",
      "287 A royal commission will begin this morni\n",
      "288 Eight people are to appear in a Swiss co\n",
      "289 There is a renewed attempt to move the d\n",
      "290 A third case of mad cow disease has been\n",
      "291 Unions and a major electricity producer \n",
      "292 Rival Afghan factions are deadlocked ove\n",
      "293 George Harrison the guitarist, songwrite\n",
      "294 Virgin Airline's first dawn flight betwe\n",
      "295 A team of Australian and Israeli scienti\n",
      "296 Today is World Aids Day and the latest f\n",
      "297 The Federal National Party has rejected \n",
      "298 A University of Canberra academic's prop\n",
      "299 Australia will take on France in the dou\n",
      "0 The national executive of the strife-tor\n",
      "1 Cash-strapped financial services group A\n",
      "2 The United States government has said it\n",
      "3 A radical armed Islamist group with ties\n",
      "4 Washington has sharply rebuked Russia ov\n",
      "5 A gay former student of a Melbourne Chri\n",
      "6 Senior members of the Saudi royal family\n",
      "7 Palestinian hired gun Abu Nidal, whose v\n",
      "8 Hunan province remained on high alert la\n",
      "9 A U.S.-British air raid in southern Iraq\n",
      "10 Iraq and Russia are close to signing a $\n",
      "11 U.S. intelligence cannot say conclusivel\n",
      "12 Drug squad detectives have asked the Pol\n",
      "13 Queensland senator Andrew Bartlett has l\n",
      "14 Very few women have been appointed to he\n",
      "15 The Bush administration has drawn up pla\n",
      "16 Beijing has abruptly withdrawn a new car\n",
      "17 The United Nations World Food Program es\n",
      "18 In Malawi, as in other countries in the \n",
      "19 The United Nations was determined that i\n",
      "20 The Iraqi capital is agog after the viol\n",
      "21 The Federal Government says changes anno\n",
      "22 The biowarfare expert under scrutiny in \n",
      "23 China said Sunday it issued new regulati\n",
      "24 Nigerian President Olusegun Obasanjo sai\n",
      "25 An Islamic high court in northern Nigeri\n",
      "26 How did 2,300 allegedly unregistered mis\n",
      "27 The Saudi Interior Ministry on Sunday co\n",
      "28 Sri Lanka's government will lift a four-\n",
      "29 A man accused of making hidden-camera fo\n",
      "30 Police are combing through videotapes tr\n",
      "31 The Federal Government did not regret it\n",
      "32 At least three Democrats are considering\n",
      "33 A young humpback whale remained tangled \n",
      "34 Prince William has told friends his moth\n",
      "35 The spectre of Osama bin Laden rose agai\n",
      "36 The Johannesburg Earth Summit is set to \n",
      "37 Robert Mugabe strengthened his hold on t\n",
      "38 They dress in black and disguise their i\n",
      "39 The real level of world inequality and e\n",
      "40 Researchers conducting the most elaborat\n",
      "41 Russia defended itself against U.S. crit\n",
      "42 Pope John Paul II urged delegates at a m\n",
      "43 The Russian defense minister said reside\n",
      "44 Australian spies listened to conversatio\n",
      "45 Batasuna, a political party that campaig\n",
      "46 The river Elbe surged to an all-time rec\n",
      "47 The European Parliament is spoiling for \n",
      "48 Australia's Commonwealth Bank on Wednesd\n",
      "49 Labor needed to distinguish itself from \n"
     ]
    }
   ],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(i,line[0:40])\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 15:15:59,346 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a list (accessible via\n",
    "``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.\n",
    "Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,\n",
    "For example, to see how many times ``penalty`` appeared in the training corpus:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2\n",
    "minutes, so use optimized Gensim with BLAS if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13236438  0.21996544 -0.05183258  0.06838805 -0.18532439  0.3244717\n",
      "  0.05605754 -0.19773999  0.08819405  0.09957626  0.31333232  0.03409576\n",
      " -0.14326398 -0.16998176 -0.04428281  0.07876049  0.09084498 -0.04030472\n",
      "  0.22755294 -0.05987298 -0.0633678  -0.16101082  0.18876418  0.01699561\n",
      " -0.04678012 -0.18106964  0.03648904  0.19383205  0.06994332 -0.11735195\n",
      " -0.17782344 -0.06777228 -0.19529793 -0.0347135  -0.00744851  0.11532683\n",
      "  0.18833125 -0.18300493  0.21450813  0.06079876 -0.09633607  0.08798445\n",
      " -0.02941022 -0.29519078  0.01633021  0.14860879 -0.05239085  0.02593254\n",
      " -0.28813702  0.02038062]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'dv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4e272c2b6023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minferred_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minferred_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'dv'"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (0): «hundreds of people have been forced to vacate their homes in the southern highlands of new south wales as strong winds today pushed huge bushfire towards the town of hill top new blaze near goulburn south west of sydney has forced the closure of the hume highway at about pm aedt marked deterioration in the weather as storm cell moved east across the blue mountains forced authorities to make decision to evacuate people from homes in outlying streets at hill top in the new south wales southern highlands an estimated residents have left their homes for nearby mittagong the new south wales rural fire service says the weather conditions which caused the fire to burn in finger formation have now eased and about fire units in and around hill top are optimistic of defending all properties as more than blazes burn on new year eve in new south wales fire crews have been called to new fire at gunning south of goulburn while few details are available at this stage fire authorities says it has closed the hume highway in both directions meanwhile new fire in sydney west is no longer threatening properties in the cranebrook area rain has fallen in some parts of the illawarra sydney the hunter valley and the north coast but the bureau of meteorology claire richards says the rain has done little to ease any of the hundred fires still burning across the state the falls have been quite isolated in those areas and generally the falls have been less than about five millimetres she said in some places really not significant at all less than millimetre so there hasn been much relief as far as rain is concerned in fact they ve probably hampered the efforts of the firefighters more because of the wind gusts that are associated with those thunderstorms»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7cfd8b123cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document ({}): «{}»\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MOST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'SECOND-MOST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'MEDIAN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'LEAST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'%s %s: «%s»\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sims' is not defined"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (244): «the reserve bank has cut official interest rates again still concerned about the slowing global economy the central bank has delivered further cut of of per cent it is the sixth rate cut for the year taking the cash rate to per cent the cut has been quickly passed on in full by all the major banks and host of smaller lenders the reserve bank says international conditions remain weak with the us and japanese economies in recession europe stalled and contractions in number of australia east asian trading partners the bank says and combined will produce the weakest period of growth since the early although the threat of an even sharper decline after the september attacks in the us has abated somewhat low interest rates and sharp drop in oil prices should see recovery start in the months ahead westpac bill evans is among those who believe the cut will not be the last the reserve bank has certainly not closed the door to further rate cuts next year he said»\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b6a419e02c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compare and print the second-most-similar document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Document ({}): «{}»\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msim_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Similar Document {}: «{}»\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "Additional Resources\n",
    "--------------------\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
    "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
    "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
    "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
    "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
