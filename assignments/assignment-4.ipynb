{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e7d9dc",
   "metadata": {},
   "source": [
    "# Assignment 4 (solutions): Attention\n",
    "\n",
    "### Due Date: Nov 22 (both sections)\n",
    "\n",
    "### Grade (100 pts, 10%)\n",
    "\n",
    "#### Your Name:Qinren Zhou\n",
    "\n",
    "#### Your EID: qz142\n",
    "\n",
    "*Note: This assignment covers material from the recording, notes, demo, and suggested readings from Lectures 5,8,9*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48900111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2390e28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem introduction\n",
    "\n",
    "In this assignment we will explore the concept of attention using Word2Vec features. Recall that in lecture-05 we used the Word2Vec algorithm to compute feature representations of english words. For every center-word - context-word pair, ${\\textbf{x}_w, \\textbf{x}_c}$, in the training set, Word2Vec tries to maximize the inner product between their feature embeddings, $\\textbf{u}_w$ and $\\textbf{v}_c$, by first computing a vector of logits as $\\textbf{z} = \\textbf{u}_w \\cdot \\textbf{V}^T \\in \\mathbb{R}^N$, and then maximizing $p(\\textbf{x}_c | \\textbf{x}_w) = \\sigma_{softmax}(\\textbf{z})$ to learn $\\textbf{U}$ and $\\textbf{V}$ via MLE. \n",
    "\n",
    "Let's assume we are given a sequence of words:\n",
    "\n",
    "$$X = \\{ \\textbf{x}^{(1)}, \\textbf{x}^{(2)}, \\textbf{x}^{(3)}, \\textbf{x}^{(4)}, \\dots, \\textbf{x}^{(T)} \\}$$\n",
    "\n",
    "a corresponding sequence of center-word feature representations:\n",
    "\n",
    "$$U = \\{ \\textbf{u}^{(1)}, \\textbf{u}^{(2)}, \\textbf{u}^{(3)}, \\textbf{u}^{(4)}, \\dots, \\textbf{u}^{(T)} \\}$$\n",
    "\n",
    "and a corresponding context-word feature representations:\n",
    "\n",
    "$$V = \\{ \\textbf{v}^{(1)}, \\textbf{v}^{(2)}, \\textbf{v}^{(3)}, \\textbf{v}^{(4)}, \\dots, \\textbf{v}^{(T)} \\}$$\n",
    "\n",
    "Then, using the Word2Vec variable naming convention above, recall that in simple self attention (Lecture-09) we compute a set of *context vectors*:\n",
    "\n",
    "$$C = \\{ \\textbf{c}^{(1)}, \\textbf{c}^{(2)}, \\textbf{c}^{(3)}, \\textbf{c}^{(4)}, \\dots, \\textbf{c}^{(T)} \\}$$ \n",
    "\n",
    "each of which is computed according to:\n",
    "\n",
    "$$\n",
    "\\textbf{c}^{(t)} = \\sum_{t'=1}^{T} \\alpha_{t,t'} \\; \\textbf{u}^{(t')} \\quad where \\quad \\alpha_{t,t'} = \\frac{ e^{ \\textbf{u}^{(t)} \\cdot \\textbf{v}^{(t')} } }{ \\sum_{t''} e^{ \\textbf{u}^{(t)} \\cdot \\textbf{v}^{(t'')} } } \\\\\n",
    "$$\n",
    "\n",
    "The attention weights make up the following (non-symmetric!) matrix:\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "\\alpha_{1,1} & \\dots & \\alpha_{1,T} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\alpha_{T,1} & \\dots & \\alpha_{T,T}\n",
    "\\end{array}\\right] \\; \\in [0,1]^{T \\times T} \\quad where \\quad \\sum_{t'} \\alpha_{t,t'} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ee28c",
   "metadata": {},
   "source": [
    "### Test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5aa8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Thomas Jefferson was an American statesman and Founding Father who served as the third president of the United States\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cc71f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Word2Vec vocab, embedding matrices\n",
    "\n",
    "In the `assignments/a4/` folder you will find three .tsv files: a vocabulary (`metadata.tsv`), center-word Word2Vec embeddings (`vectors_center.tsv`), and context-word Word2Vec embeddings (`vectors_context.tsv`). These are taking from the Lecture-05 Word2Vec demo. The code below loads the center and context word embeddings for the above test sentence into two numpy arrays: $U$, and $V$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37151d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2321\n",
      "embed dim: 15\n",
      "center embedding lookup matrix shape: (15, 2321)\n",
      "context embedding lookup matrix shape: (15, 2321)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_matrix(fpath):\n",
    "    matrix = []\n",
    "    with open(fpath, 'r') as fd:\n",
    "        tsv = fd.read()\n",
    "    for line in tsv.split('\\n'):\n",
    "        row = []\n",
    "        for value in line.split('\\t'):\n",
    "            row.append(float(value))\n",
    "        matrix.append(row)\n",
    "    return np.array(matrix)\n",
    "\n",
    "def load_vocab(fpath) -> dict: # metadata.tsv\n",
    "    with open(fpath, \"r\") as fd:\n",
    "        tsv = fd.read()\n",
    "    vocab = {}\n",
    "    for line in tsv.split('\\n'):\n",
    "        vocab[line.strip()] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = load_vocab(\"metadata.tsv\")\n",
    "U_lookup = load_matrix(\"vectors_center.tsv\").T\n",
    "V_lookup = load_matrix(\"vectors_context.tsv\").T\n",
    "\n",
    "N = len(vocab)\n",
    "D = len(U_lookup)\n",
    "\n",
    "print(f\"vocab size: {N}\")\n",
    "print(f\"embed dim: {D}\")\n",
    "\n",
    "print(f\"center embedding lookup matrix shape: {U_lookup.shape}\")\n",
    "print(f\"context embedding lookup matrix shape: {V_lookup.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9701d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get('president', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f943bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indices: [2092, 1075, 2321, 2321, 79, 2321, 2321, 795, 2321, 2321, 2321, 2321, 2321, 2321, 1622, 2321, 2321, 2158, 1987]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Thomas Jefferson was an American statesman and Founding Father who served as the third president of the United States\"\n",
    "word_list = sentence.lower().split(\" \")\n",
    "X = [vocab.get(word, N) for word in word_list]\n",
    "\n",
    "print(f\"Word indices: {X}\")\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c03aec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center word embedding lookup matrix w/oov embed added shape: (15, 2322)\n",
      "context word embedding lookup matrix w/oov embed added shape: (15, 2322)\n"
     ]
    }
   ],
   "source": [
    "# Assign OOV words low attention weights\n",
    "u_oov = 0.1 * np.mean(U_lookup, axis=1, keepdims=True)\n",
    "v_oov = 0.1 * np.mean(V_lookup, axis=1, keepdims=True) \n",
    "# Add the OOV embeddings to the lookup matrices\n",
    "U_lookup = np.concatenate((U_lookup, u_oov), axis=1)\n",
    "V_lookup = np.concatenate((V_lookup, v_oov), axis=1)\n",
    "U_lookup.shape, V_lookup.shape\n",
    "\n",
    "print(f\"center word embedding lookup matrix w/oov embed added shape: {U_lookup.shape}\")\n",
    "print(f\"context word embedding lookup matrix w/oov embed added shape: {U_lookup.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e61bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence of center-word embeddings shape: (15, 19)\n",
      "sequence of context-word embeddings shape: (15, 19)\n"
     ]
    }
   ],
   "source": [
    "# Center-word embedding sequence of X\n",
    "U = U_lookup[:, X]\n",
    "# Context-word embedding sequence of X\n",
    "V = V_lookup[:, X]\n",
    "\n",
    "print(f\"sequence of center-word embeddings shape: {U.shape}\")\n",
    "print(f\"sequence of context-word embeddings shape: {U.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85151a9a",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83eaa38",
   "metadata": {},
   "source": [
    "### (40 pts) Q1: Attention weights\n",
    "\n",
    "1. Compute the attention weight matrix $\\textbf{A}$ for this test sentence using the definitions provided in the intro (and in Lecture-09 as needed).\n",
    "\n",
    "*Hint: You can use the batched `softmax()` function provided below to compute A.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1888a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes a softmax over each row of Z\n",
    "    \"\"\"\n",
    "    Z_exp = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    partition = np.sum(Z_exp, axis=1, keepdims=True)\n",
    "    return Z_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b81fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 19\n"
     ]
    }
   ],
   "source": [
    "# len of sentence: 19\n",
    "# dim len : 15\n",
    "A = []\n",
    "for t in range(len(X)):\n",
    "    ut  = U[:,t]\n",
    "    lis = []\n",
    "    for tt in range(len(X)):\n",
    "        vtt = V[:,tt]\n",
    "        lis.append(np.exp(np.dot(ut,vtt)))\n",
    "    sum_tmp = sum(lis)\n",
    "    lis = [round(i/sum_tmp,6) for i in lis]\n",
    "    A.append(lis)\n",
    "    \n",
    "print(len(A),len(A[0]))\n",
    "#print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c0123",
   "metadata": {},
   "source": [
    "### (40 pts) Q2: Attention block\n",
    "\n",
    "1. Now compute the attention block $C$ using the attention weights that you've computed\n",
    "\n",
    "2. Compute the L2 norm of each of the resultant context vectors. How do the magnitudes compare? Do the same for the center-word embeddings in the preceeding layer. Is there a logical explanation for this?\n",
    "\n",
    "*Hint: You need to compute $\\textbf{c}^{(t)} \\; \\forall t \\in \\{1, \\dots, T\\}$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15395f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 19\n"
     ]
    }
   ],
   "source": [
    "def list_add(lisA,lisB):\n",
    "    res = []\n",
    "    for i in range(len(lisA)):\n",
    "        res.append(lisA[i] + lisB[i])\n",
    "    return res\n",
    "\n",
    "\n",
    "C = []\n",
    "for t in range(len(U[0])):      # loop U by col\n",
    "    ct = [0]*len(U[0])\n",
    "    for tt in range(len(U[0])): # loop each element in A\n",
    "        a = A[t][tt]            # a(t,t')\n",
    "        tmp = [i * a for i in U[:,tt]]  # a(t,t') * U{t}\n",
    "        ct = list_add(tmp, ct)          # sum, get C(t)\n",
    "    C.append(ct)\n",
    "C = np.array(C)\n",
    "C = C.T\n",
    "print(len(C),len(C[0]))\n",
    "#print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "402778da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.155972097134811\n",
      "1 2.2276660321735493\n",
      "2 1.1225692169726431\n",
      "3 1.1225692169726431\n",
      "4 2.27455988446378\n",
      "5 1.1225692169726431\n",
      "6 1.1225692169726431\n",
      "7 2.3455183239303006\n",
      "8 1.1225692169726431\n",
      "9 1.1225692169726431\n",
      "10 1.1225692169726431\n",
      "11 1.1225692169726431\n",
      "12 1.1225692169726431\n",
      "13 1.1225692169726431\n",
      "14 2.18157218083374\n",
      "15 1.1225692169726431\n",
      "16 1.1225692169726431\n",
      "17 2.4893443419038395\n",
      "18 2.482837573127786\n"
     ]
    }
   ],
   "source": [
    "# L2 for C \n",
    "C = C.T\n",
    "for i in range(len(C)):\n",
    "    vec = np.array(C[i])\n",
    "    vec = np.linalg.norm(vec)\n",
    "    print(i,vec)\n",
    "# each line represent the L2 normal for attention weights C(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb9e18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.7284433320768997\n",
      "1 2.971433701326006\n",
      "2 0.18668971616635238\n",
      "3 0.18668971616635238\n",
      "4 2.2807288702742743\n",
      "5 0.18668971616635238\n",
      "6 0.18668971616635238\n",
      "7 2.6194201449887724\n",
      "8 0.18668971616635238\n",
      "9 0.18668971616635238\n",
      "10 0.18668971616635238\n",
      "11 0.18668971616635238\n",
      "12 0.18668971616635238\n",
      "13 0.18668971616635238\n",
      "14 2.9704852555359977\n",
      "15 0.18668971616635238\n",
      "16 0.18668971616635238\n",
      "17 2.535290847011032\n",
      "18 2.532197722371995\n"
     ]
    }
   ],
   "source": [
    "U = U.T \n",
    "for i in range(len(U)):\n",
    "    vec = np.array(U[i])\n",
    "    vec = np.linalg.norm(vec)\n",
    "    print(i,vec)\n",
    "# each line represent the L2 normal for center-word embeddings U(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b63e0d",
   "metadata": {},
   "source": [
    "After L2 normal, the result of each attention weights become more stable, compared to visible fluctuations in center-word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b8f60",
   "metadata": {},
   "source": [
    "### (20 pts) Q3: Visualize the attention weights\n",
    "\n",
    "Now that we've computed a simple self attention transformation of our Word2Vec sequence representation, it's time to visually examine what this attention mechanism affords us. First recognize that all values in $\\textbf{A}$ are in the range $[0,1]$, and that each row is normalized. Therefore, we really just need to generate a heatmap of this matrix, keeping in mind that the matrix indices correspond directly to the positions in the sequence.\n",
    "\n",
    "1. Generate the heatmap in the cell provided below\n",
    "\n",
    "2. Briefly describe/explain what you see in the heatmap.\n",
    "\n",
    "3. Why is our attention matrix not symmetric?\n",
    "\n",
    "*Hint: You can generate a heatmap of the attention weights by calling `plot_attention_weights(A)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a1ad7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_attention_weights(att_matrix):\n",
    "    fig = px.imshow(A, x=word_list, y=word_list, width=20, height=20)\n",
    "    fig.update_layout(width=800, height=800)\n",
    "    print(sentence)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01db8327",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0000049999999998, 0.9999969999999998, 0.999997, 0.999997, 1.0000040000000001, 0.999997, 0.999997, 0.999999, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.9999990000000001, 1.000001]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "imshow() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-cd9df1057d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_sum\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# it should be near 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplot_attention_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-fd2eca3136b5>\u001b[0m in \u001b[0;36mplot_attention_weights\u001b[0;34m(att_matrix)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_attention_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: imshow() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# make sure each element in A is in the range  [0,1] and each row is normalized\n",
    "lis  = []\n",
    "for i in A:\n",
    "    line_sum = 0\n",
    "    for j in i:\n",
    "        line_sum += j\n",
    "    lis.append(line_sum)  # it should be near 1\n",
    "print(lis)\n",
    "plot_attention_weights(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7714536",
   "metadata": {},
   "source": [
    "The words [Thomas,Jefferson,founding,president], pay more attention to the word 'United States' and 'American' .\n",
    "Intuitively understand, because Thomas Jefferson often appears with the  'United States' or 'American' in the text,\n",
    "And this verb 'founding' generally modify the nouns of a country or organization\n",
    "as well as the word 'presidential' is bound to the concept of a country or organization.\n",
    "\n",
    "Besides, proper nouns and key verbs tend to get more attention, while other part-of-speech words are less important in sentences.\n",
    "\n",
    "As for synonyms, words between common phrase groups also share more attention. \n",
    "In Word2vec, words with similar context will be closer together in the embedded representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052b825",
   "metadata": {},
   "source": [
    "In terms of why attention matrix not symmetric. The intuitive understanding is that in a sentence, \n",
    "the attention of A word to B word is not necessarily equal to the attention of B word to A word. \n",
    "For example, for words 'founding', Ignore other words and assuming the word 'American' and 'United States' share same attention, 0.5 by 0.5 for example.\n",
    "However, when considering the word 'American', the distance between 'American' and 'United States' will be much closer compared to 'American' and 'United States'\n",
    "Thus, the word 'American' may pay more attention to 'United States', 0.3 by 0.7 for example.\n",
    "\n",
    "Another way to understand mathematics is to change the matrix to calculate row by row, and the sum of each row is 1. When calculating the first row (that is, the attention of the first word), it has nothing to do with the attention of other words. , Calculated separately and independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c17fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
